'''
This is a simple snakemake workflow for running usher, matUtils, and ripples

Include in the current working directory:
    1. a fasta file with SARS-CoV-2 genome sequences: [user_fa] (below)
    2. the Snakefile
    3. the environment file, usher.yaml

Users can run each workflow as:

UShER: add samples to the latest public MAT
    snakemake --use-conda --cores [num threads] --config FASTA="[user_fa]" RUNTYPE="usher"
matUtils: extract subtrees in auspice.us compatible json format using matUtils
    snakemake --use-conda --cores [num threads] --config FASTA="[user_fa]" RUNTYPE="matUtils"
ripples: detect recombinants in the ancestry of the user-supplied samples
    snakemake --use-conda --cores [num threads] --config FASTA="[user_fa]" RUNTYPE="ripples"
translate: translate all mutations to AA affecting sites
    snakemake --use-conda --cores [num threads] --config FASTA="[user_fa]" RUNTYPE="translate"
taxodium: output taxodium format protobuf for visualization of the big tree
    snakemake --use-conda --cores [num threads] --config FASTA="[user_fa]" RUNTYPE="taxodium"
introduce: run introduce on the user provided samples to identify clusters
    snakemake --use-conda --cores [num threads] --config FASTA="[user_fa]" RUNTYPE="introduce"
systematic: search for possible systematic errors in your added samples by outputing a list of sites whose parsimony score increased
    snakemake --use-conda --cores [num threads] --config FASTA="[user_fa]" RUNTYPE="systematic"

For each run type, the usher.yaml environment file must be present in the working directory.

There are many other advanced options, for more information, please see the wiki:

    https://usher-wiki.readthedocs.io/en/latest/

rucorbet@ucsc.educ


'''

rule all:
    input:
        config["RUNTYPE"]

rule download_reference:
    output:
        ref = "wuhCor1.fa"
    conda:
        "usher.yaml"
    shell:
        "wget https://hgdownload.soe.ucsc.edu/goldenPath/wuhCor1/bigZips/wuhCor1.fa.gz && gunzip wuhCor1.fa.gz"

rule align_seqs:
    input:
        rules.download_reference.output.ref,
        config["FASTA"]
    output:
        aln = "aligned_seqs.fa"
    conda:
        "usher.yaml"
    threads:
        64
    shell:
        "mafft --thread {threads} --auto --keeplength --addfragments {input[1]} {input[0]} > aligned_seqs.fa"

rule download_masking:
    output:
        probvcf = "problematic_sites_sarsCov2.vcf"
    conda:
        "usher.yaml"
    shell:
        "wget https://raw.githubusercontent.com/W-L/ProblematicSites_SARS-CoV2/master/problematic_sites_sarsCov2.vcf"

rule create_vcf:
    input:
        rules.download_masking.output.probvcf,
        rules.align_seqs.output.aln
    output:
        alnvcf = "aligned_seqs.vcf",
    conda:
        "usher.yaml"
    shell:
        "faToVcf -maskSites={input[0]} {input[1]} aligned_seqs.vcf"

rule download_protobuf:
    output:
        dataset = "public-latest.all.masked.pb.gz" 
    conda:
        "usher.yaml"
    shell:
        "wget http://hgdownload.soe.ucsc.edu/goldenPath/wuhCor1/UShER_SARS-CoV-2/public-latest.all.masked.pb.gz"

rule usher:
    input:
        rules.download_protobuf.output.dataset,
        rules.create_vcf.output.alnvcf
    conda:
        "usher.yaml"
    output:
        updateddataset = "user_seqs.pb",
        temp = temp("usher"),
        treenh = "final-tree.nh",
        mutpath = "mutation-paths.txt"
    threads:
        threads = 64
    shell:
        "usher --threads {threads} --load-mutation-annotated-tree {input[0]} --vcf {input[1]} --save-mutation-annotated-tree user_seqs.pb > usher"

rule get_sample_ids:
    input:
        config["FASTA"]
    conda:
        "usher.yaml"
    output:
        usersamples = "user_samples.txt"
    shell:
        "grep -e '>' {input[0]} | perl -pi -e 's/>//' > user_samples.txt"

rule introduce:
    input:
        rules.usher.output.updateddataset,
        rules.get_sample_ids.output.usersamples
    conda:
        "usher.yaml"
    output:
        temp = temp("introduce"),
        inferredintros = "user_seqs.introductions.txt",
        clusters = "user_seqs.clusters.txt"
    threads:
        64
    shell:
        "matUtils introduce -T {threads} -i {input[0]} --population-samples {input[1]} --cluster-output user_seqs.clusters.txt --full-output user_seqs.introductions.txt > introduce"
        
rule download_metadata :
    conda:
        "usher.yaml"
    output:
        metadata = "public-latest.metadata.tsv"
    shell:
        "wget http://hgdownload.soe.ucsc.edu/goldenPath/wuhCor1/UShER_SARS-CoV-2/public-latest.metadata.tsv.gz && gunzip public-latest.metadata.tsv.gz"

rule download_gtf :
    conda:
        "usher.yaml"
    output:
        gtffile = "ncbiGenes.gtf"
    shell:
        "wget http://hgdownload.soe.ucsc.edu/goldenPath/wuhCor1/bigZips/genes/ncbiGenes.gtf.gz && gunzip ncbiGenes.gtf.gz"

rule translate :
    input:
        rules.usher.output.updateddataset,        
        rules.download_gtf.output.gtffile,
        rules.download_reference.output.ref
    conda:
        "usher.yaml"
    threads:
        64
    output:
        translation = "user_seqs.translation.tsv",
        temp = temp("translate")
    shell:
        "matUtils summary -t user_seqs.translation.tsv -i {input[0]} -g {input[1]} -f {input[2]} > translate"

rule extract_subtrees :
    input :
        rules.usher.output.updateddataset,
        rules.get_sample_ids.output.usersamples,
        rules.download_metadata.output.metadata,
        rules.translate.output.translation
    conda:
        "usher.yaml"
    output :
        temp = temp("matUtils"),
        output = "subtree-assignments.tsv"
    threads:
        64
    shell :
        "matUtils extract -T {threads} -i {input[0]} -s {input[1]} -M {input[2]},{input[3]} -N 500 -j user > matUtils"

rule taxodium :
    input :
        rules.usher.output.updateddataset,
        rules.download_gtf.output.gtffile,
        rules.download_reference.output.ref,
        rules.download_metadata.output.metadata
    output:
        "user_seqs.taxodium.pb",
        temp("taxodium")
    conda:
        "usher.yaml"
    threads:
        64
    shell:
        "matUtils extract -i {input[0]} -T {threads} -l {output[0]} -g {input[1]} -f {input[2]} -M {input[3]} > taxodium"

rule ripples :
    input :
        rules.usher.output.updateddataset,
        rules.get_sample_ids.output.usersamples
    output:
        temp("ripples")
    conda:
        "usher.yaml"
    threads:
        64
    shell:
        "ripples -i {input[0]} -s {input[1]} -T {threads} > ripples"
        
rule extract_parsimony_difference :
    input :
        rules.usher.output.updateddataset,
        rules.download_protobuf.output.dataset
    output :
        temp("parsimony_user_data.txt"),
        temp("parsimony_public.txt"),
        temp("samples_public.txt"),
        temp("samples_total.txt"),
        temp("parsimony_diffs.txt")
    conda :
        "usher.yaml"
    shell :
        """
        matUtils summary -i {input[0]} -s {output[3]} -m {output[0]}
        matUtils summary -i {input[1]} -s {output[2]} -m {output[1]}
        join {output[1]} {output[0]} | awk '$2 != $3' | awk '{{ print $1, $2, $3, $3-$2 }}' | sort -nrk4,4 >> {output[4]}
        """
	
rule create_report :
    input :
        "samples_public.txt",
        "samples_total.txt",
        "parsimony_diffs.txt"
    output :
        "parsimony_report.txt",
        temp(touch("systematic"))
    run :
        ### get total samples in the tree after addition
        out = subprocess.Popen(["wc", "-l","samples_total.txt"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        stdout,stderr = out.communicate()
        total_samples = float( stdout.split()[0] )

        ### get total samples in the tree before addition
        out = subprocess.Popen(["wc", "-l","samples_public.txt"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        stdout,stderr = out.communicate()
        public_samples = float( stdout.split()[0] )

        ## user samples is the difference
        user_samples = total_samples - public_samples

        ## write to file
        with open("parsimony_report.txt", "w") as pars_out:

            ## now capture the relevant positions
            with open("parsimony_diffs.txt", "r") as file:

                ### print the header
                print ("variant\tparsimony_public\tparsimony_total\tparsimony_user\tparsimony_public/sample\tparsimony_user/sample", file=pars_out )

                ### each line
                for line in file:
                    line = line.strip()
                    fields = line.split(" ")
                    print ( fields[0], fields[1], fields[2], fields[3], float(fields[1])/total_samples, float(fields[3])/user_samples, sep = "\t", file=pars_out )
